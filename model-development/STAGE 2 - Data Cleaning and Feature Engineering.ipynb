{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Import Requirements </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: impyute in /home/nbuser/anaconda3_420/lib/python3.5/site-packages (0.0.8)\n",
      "Requirement already satisfied: numpy in /home/nbuser/anaconda3_420/lib/python3.5/site-packages (from impyute) (1.16.3)\n",
      "Requirement already satisfied: scikit-learn in /home/nbuser/anaconda3_420/lib/python3.5/site-packages (from impyute) (0.21.2)\n",
      "Requirement already satisfied: scipy in /home/nbuser/anaconda3_420/lib/python3.5/site-packages (from impyute) (1.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/nbuser/anaconda3_420/lib/python3.5/site-packages (from scikit-learn->impyute) (0.12.5)\n",
      "\u001b[33mWARNING: You are using pip version 19.1, however version 19.1.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already up-to-date: scikit-learn in /home/nbuser/anaconda3_420/lib/python3.5/site-packages (0.21.2)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.11.0 in /home/nbuser/anaconda3_420/lib/python3.5/site-packages (from scikit-learn) (1.16.3)\n",
      "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /home/nbuser/anaconda3_420/lib/python3.5/site-packages (from scikit-learn) (0.12.5)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=0.17.0 in /home/nbuser/anaconda3_420/lib/python3.5/site-packages (from scikit-learn) (1.1.0)\n",
      "\u001b[33mWARNING: You are using pip version 19.1, however version 19.1.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install impyute\n",
    "!pip install -U scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import matplotlib.pyplot as plt\n",
    "from impyute.imputation.cs import fast_knn\n",
    "from impyute.imputation.cs import mice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Load Necessary Data </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2566 entries, 0 to 2565\n",
      "Columns: 132 entries, date to firm\n",
      "dtypes: float64(130), object(2)\n",
      "memory usage: 2.6+ MB\n"
     ]
    }
   ],
   "source": [
    "alldata = pd.read_csv(\"sp500alldata.csv\", error_bad_lines=False)\n",
    "alldata.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 505 entries, 0 to 504\n",
      "Data columns (total 3 columns):\n",
      "Symbol    505 non-null object\n",
      "Name      505 non-null object\n",
      "Sector    505 non-null object\n",
      "dtypes: object(3)\n",
      "memory usage: 11.9+ KB\n"
     ]
    }
   ],
   "source": [
    "constdata = pd.read_csv(\"constituents_csv.csv\", error_bad_lines=False)\n",
    "constdata.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 58 entries, 0 to 57\n",
      "Data columns (total 4 columns):\n",
      "Date       58 non-null int64\n",
      "Value_x    58 non-null float64\n",
      "Value_y    58 non-null float64\n",
      "Value      43 non-null float64\n",
      "dtypes: float64(3), int64(1)\n",
      "memory usage: 1.9 KB\n"
     ]
    }
   ],
   "source": [
    "econdata = pd.read_csv(\"annualecondata.csv\", error_bad_lines=False)\n",
    "econdata.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 221 entries, 0 to 220\n",
      "Data columns (total 3 columns):\n",
      "Date              221 non-null int64\n",
      "Index Value       221 non-null float64\n",
      "Standard Error    221 non-null float64\n",
      "dtypes: float64(2), int64(1)\n",
      "memory usage: 5.3 KB\n"
     ]
    }
   ],
   "source": [
    "stockconf = pd.read_csv(\"stockconfidencedata.csv\", error_bad_lines=False)\n",
    "stockconf.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Prepare Consolidated Dataset </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> STEP 1: Dimension Cleaning </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While most of the data is 5-year worth of yearly information, some errenous entries have far more frequent information - monthly, quarterly etc., owing to irregularities in data collection. Since dividend payments are considered yearly, such errenous values must be dealt with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "alldata.sort_values(by=['firm', 'date'], ascending = [True, True], inplace=True)\n",
    "alldata.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VALID         2560\n",
       "Unnamed: 1       6\n",
       "Name: date, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alldata.date.apply(lambda s: 'VALID' if str(s)[0].isnumeric() else s).value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "firms = alldata[alldata.date=='Unnamed: 1'].firm\n",
    "alldata = alldata[(~(alldata.firm.isin(firms)))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['EVHC', 'HBI', 'NFX', 'NWS', 'SCG', 'UA', 'YUM'], dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter = alldata.date.apply(lambda s: int(s.split(\"-\")[0]))\n",
    "rmv = alldata.loc[(counter.diff().shift(-1)==0) & (alldata.firm == alldata.firm.shift(-1)), ['date', 'firm']]\n",
    "rmv.firm.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "alldata.drop(rmv.index, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> STEP 2: Missing Value Deletions </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first analyze the number of missing values across columns and decide to drop any above 60%. As seen below, 21 of the 168 columns meet this requirement. Out of these, however, investments in research and development is considered an important factor for a firm, and is hence kept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Operating expenses                      1.0\n",
       "Research and development                1.0\n",
       "Other                                   1.0\n",
       "Earnings per share                      1.0\n",
       "Weighted average shares outstanding     1.0\n",
       "Assets                                  1.0\n",
       "Current assets                          1.0\n",
       "Cash                                    1.0\n",
       "Short-term investments                  1.0\n",
       "Non-current assets                      1.0\n",
       "Property, plant and equipment           1.0\n",
       "Equity and other investments            1.0\n",
       "Prepaid pension benefit                 1.0\n",
       "Liabilities and stockholders' equity    1.0\n",
       "Liabilities                             1.0\n",
       "Current liabilities                     1.0\n",
       "Non-current liabilities                 1.0\n",
       "Capital leases                          1.0\n",
       "Pensions and other benefits             1.0\n",
       "Minority interest                       1.0\n",
       "Stockholders' equity                    1.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = alldata.apply(lambda s: 1 if (sum(s.isnull() * 1)) > 0.6 * len(s) else np.nan).dropna()\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "coldel = np.delete(x.index.values, np.where(x.index.values=='Research and development')[0])\n",
    "alldata.drop(coldel, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NFX    535.0\n",
       "NWS    535.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We now analyze the number of null values per company. While most companies have 5 rows (5 years), some have fewer and so we need to make\n",
    "#sure that the firms where np.nan exceeds 60% of values are removed. We can use value_counts() to retrieve how many rows a firm has\n",
    "rownum = alldata.firm.value_counts()\n",
    "rownum = rownum[alldata.firm.unique()]\n",
    "#Having retrived correct row counts, we now get null numbers for each company\n",
    "nullnum = alldata.apply(lambda s: sum(s.isnull() * 1), axis=1)\n",
    "nullnum = np.cumsum(nullnum)\n",
    "changeorg = nullnum.where(alldata.firm != alldata.firm.shift(1)).ffill().fillna(0)\n",
    "nullnum = nullnum - changeorg\n",
    "nullnum = nullnum.where(alldata.firm != alldata.firm.shift(-1)).dropna()\n",
    "nullnum.index = alldata.firm.unique()\n",
    "nval = nullnum[nullnum >= 0.6 * rownum * 148]\n",
    "nval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "firmstodrop = nval.index.values\n",
    "indices = alldata[alldata.firm.isin(firmstodrop)].index.values\n",
    "alldata.drop(indices, axis=0, inplace=True)\n",
    "alldata.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Special adjustment based on quality, relevance and repetition of data involved\n",
    "#The columns involved below are  repetitions or correlations of existing financial metrics, thus crowding analysis\n",
    "badcols = ['Net income from continuing operations', 'Net income available to common shareholders', \n",
    "           'Cash and cash equivalents', 'Current ratio','Debt to Equity','ROE', 'ROIC', \n",
    "           'Receivables Turnover','Return on Tangible Assets', \n",
    "           'Tangible Asset Value','Working Capital']\n",
    "alldata.drop(badcols, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> STEP 3: Merge Supplementary Data </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We must combine the main S&P500 financial dataset with the economics, stock confidence and sector data that was collected simultaneously. Sector data is merged on firm, annual economics on year and stock confidence on month. Relevant features are temporarily added to facilitate such merges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "constdata.rename(columns={'Symbol':'firm'}, inplace=True)\n",
    "alldata = alldata.merge(constdata[['firm', 'Sector']], on='firm', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = econdata.columns.values\n",
    "vals = ['Year','GDP', 'GNI', 'Market_Cap']\n",
    "econdata.rename(columns = dict(zip(cols, vals)), inplace=True)\n",
    "econdata.Year = econdata.Year.apply(lambda s: int(s/100))\n",
    "alldata['Year'] = alldata.date.apply(lambda s: int(s.split('-')[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Front fill to latest economics entry required before merging\n",
    "latestecon = econdata.sort_values(by='Year', ascending=False).iloc[0]\n",
    "curtime = alldata.Year.max()\n",
    "newvals = econdata.loc[np.array([0]).repeat(curtime - latestecon.Year)].copy()\n",
    "adj = pd.Series(np.arange(curtime - latestecon.Year) + 1)\n",
    "newvals.reset_index(drop=True, inplace=True)\n",
    "newvals['Year'] = newvals['Year'] + adj\n",
    "econdata = econdata.append(newvals)\n",
    "econdata.sort_values(by='Year', ascending=False, inplace=True)\n",
    "econdata = econdata[econdata.Year >= alldata.Year.min()]\n",
    "econdata.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "econdata.Year = econdata.Year.astype('int')\n",
    "alldata = alldata.merge(econdata, on='Year', how='left')\n",
    "alldata.drop(['Year'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yearshift(base, amt):\n",
    "    mth = base % 100\n",
    "    year = (base - mth)/100\n",
    "    mth = mth + amt\n",
    "    if mth > 12:\n",
    "        year = year + (mth - (mth % 12))/12\n",
    "        mth = mth % 12\n",
    "    return int(year * 100 + mth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mthdiff (val1, val2):\n",
    "    endyear = int(val1/100)\n",
    "    endmonth = val1 % 100\n",
    "    startyear = int(val2/100)\n",
    "    startmonth = val2 % 100\n",
    "    return (endyear - startyear) * 100 + (endmonth - startmonth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nbuser/anaconda3_420/lib/python3.5/site-packages/ipykernel/__main__.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/nbuser/anaconda3_420/lib/python3.5/site-packages/ipykernel/__main__.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "#Front fill to latest stock confidence values required before merging\n",
    "stockconf.rename(columns={'Date':'date'}, inplace=True)\n",
    "alldata.date = alldata.date.apply(lambda s: int(s.split('-')[0] + s.split('-')[1]))\n",
    "conflatest = stockconf.date.max()\n",
    "curperiod = alldata.date.max()\n",
    "newvals = stockconf.loc[np.array([stockconf.loc[stockconf.date==conflatest].index[0]]).repeat(mthdiff(curperiod, conflatest))]\n",
    "newvals.reset_index(drop=True, inplace=True)\n",
    "adj = pd.Series(np.arange(mthdiff(curperiod, conflatest)) + 1)\n",
    "newvals['date'] = newvals['date'].astype('str') + ' ' + adj.astype('str')\n",
    "newvals['date'] = newvals['date'].apply(lambda s: yearshift(int(s.split()[0]), int(s.split()[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 2324 entries, 0 to 2323\n",
      "Columns: 107 entries, date to Standard Error\n",
      "dtypes: float64(104), int64(1), object(2)\n",
      "memory usage: 1.9+ MB\n"
     ]
    }
   ],
   "source": [
    "stockconf = stockconf.append(newvals)\n",
    "stockconf.sort_values(by='date', ascending=False)\n",
    "stockconf = stockconf[stockconf.date >= alldata.date.min()]\n",
    "stockconf.reset_index(drop=True, inplace=True)\n",
    "alldata = alldata.merge(stockconf, on='date', how='left')\n",
    "alldata.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Missing Value Imputation </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The strategy for imputation we decided to adopt was three-pronged. First, we attempt to calculate income and balance sheet values (not ratios), if possible from other corresponding columns, like Revenue = Grossprofit + Cost of revenue, etc. We repeat this for income statement and balance sheet variables (since such corrections exist), until no better values can be figured. For remaining null values, in the second stage, we fill all nulls in a column with a company-by-company average of non-null figures. In the third stage, if such missing values continue to persist, we  impute them with  average value from their sector of operations. Finally, we re-calculate all ratios from the new information gained by performing the three stages on income/balance sheet data, and any ratio outside the scope of such calculation is filled again by sklearn's Imputer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> STEP 1: Internal Calculations </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8544\n",
      "8347\n",
      "8347\n",
      "Income values done\n"
     ]
    }
   ],
   "source": [
    "#Income statement variables\n",
    "#We repeat it till it is able to make no more difference\n",
    "oldnullinc = np.inf\n",
    "curnullinc = alldata.apply(lambda s: sum(s.isnull()))[:13].values.sum()\n",
    "while curnullinc < oldnullinc:\n",
    "    alldata.loc[alldata.Revenue.isnull(), \"Revenue\"] = alldata[\"Gross profit\"] + alldata[\"Cost of revenue\"]\n",
    "    alldata.loc[alldata['Cost of revenue'].isnull(), \"Cost of revenue\"] = alldata[\"Revenue\"] - alldata[\"Gross profit\"]\n",
    "    alldata.loc[alldata['Gross profit'].isnull(), \"Gross profit\"] = alldata[\"Revenue\"] - alldata[\"Cost of revenue\"]\n",
    "    alldata.loc[alldata['Gross profit'].isnull(), \"Gross profit\"] = alldata[\"Operating income\"] + alldata[\"Total operating expenses\"]\n",
    "    alldata.loc[alldata['Total operating expenses'].isnull(), \"Total operating expenses\"] = alldata[\"Gross profit\"] - alldata[\"Operating income\"]\n",
    "    alldata.loc[alldata['Operating income'].isnull(), \"Operating income\"] = alldata[\"Gross profit\"] - alldata[\"Total operating expenses\"]\n",
    "    alldata.loc[alldata['Operating income'].isnull(), \"Operating income\"]  = alldata[\"Income before taxes\"] - alldata['Other income (expense)'] + alldata['Interest Expense']\n",
    "    alldata.loc[alldata['Interest Expense'].isnull(), \"Interest Expense\"] = alldata['Operating income'] + alldata['Other income (expense)'] - alldata['Income before taxes']\n",
    "    alldata.loc[alldata['Other income (expense)'].isnull(), \"Other income (expense)\"] = alldata[\"Income before taxes\"] + alldata[\"Interest Expense\"] - alldata[\"Operating income\"]\n",
    "    alldata.loc[alldata['Income before taxes'].isnull(), \"Income before taxes\"] = alldata['Operating income'] - alldata['Interest Expense'] + alldata['Other income (expense)']\n",
    "    alldata.loc[alldata['Income before taxes'].isnull(), \"Income before taxes\"] = alldata['Net income'] + alldata['Provision for income taxes']\n",
    "    alldata.loc[alldata['Provision for income taxes'].isnull(), \"Provision for income taxes\"] = alldata[\"Income before taxes\"] - alldata[\"Net income\"]\n",
    "    alldata.loc[alldata['Net income'].isnull(), \"Net income\"] = alldata['Income before taxes'] - alldata['Provision for income taxes']\n",
    "    oldnullinc = curnullinc\n",
    "    curnullinc = alldata.apply(lambda s: sum(s.isnull()))[:13].values.sum()\n",
    "    print(curnullinc)\n",
    "\n",
    "print(\"Income values done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18661\n",
      "18559\n",
      "18559\n",
      "balance sheet variables done\n"
     ]
    }
   ],
   "source": [
    "#Balance sheet variables\n",
    "#We repeat till it is able to make no more difference, similarly as above\n",
    "oldnullinc = np.inf\n",
    "curnullinc = alldata.apply(lambda s: sum(s.isnull()))[16:48].values.sum()\n",
    "while curnullinc < oldnullinc:\n",
    "    alldata.loc[alldata['Total current assets'].isnull(), \"Total current assets\"] = alldata['Total cash'] + alldata['Receivables'] + alldata['Inventories'] + alldata['Prepaid expenses'] + alldata['Other current assets']\n",
    "    alldata.loc[alldata['Total cash'].isnull(), \"Total cash\"] = alldata['Total current assets'] - alldata['Receivables'] - alldata['Inventories'] - alldata['Prepaid expenses'] - alldata['Other current assets']\n",
    "    alldata.loc[alldata['Receivables'].isnull(), \"Receivables\"] = alldata['Total current assets'] - alldata['Total cash'] - alldata['Inventories'] - alldata['Prepaid expenses'] - alldata['Other current assets']\n",
    "    alldata.loc[alldata['Inventories'].isnull(), \"Inventories\"] = alldata['Total current assets'] - alldata['Receivables'] - alldata['Total cash'] - alldata['Prepaid expenses'] - alldata['Other current assets']\n",
    "    alldata.loc[alldata['Prepaid expenses'].isnull(), \"Prepaid expenses\"] = alldata['Total current assets'] - alldata['Receivables'] - alldata['Inventories'] - alldata['Total cash'] - alldata['Other current assets']\n",
    "    alldata.loc[alldata['Other current assets'].isnull(), \"Other current assets\"] = alldata['Total current assets'] - alldata['Receivables'] - alldata['Inventories'] - alldata['Prepaid expenses'] - alldata['Total cash']\n",
    "    alldata.loc[alldata['Gross property, plant and equipment'].isnull(), \"Gross property, plant and equipment\"] = alldata['Net property, plant and equipment'] - alldata['Accumulated Depreciation']\n",
    "    alldata.loc[alldata['Accumulated Depreciation'].isnull(), \"Accumulated Depreciation\"] = alldata['Net property, plant and equipment'] - alldata['Gross property, plant and equipment']\n",
    "    alldata.loc[alldata['Net property, plant and equipment'].isnull(), 'Net property, plant and equipment'] = alldata['Gross property, plant and equipment'] + alldata['Accumulated Depreciation']\n",
    "    alldata.loc[alldata['Net property, plant and equipment'].isnull(), 'Net property, plant and equipment'] = alldata['Total non-current assets'] - alldata['Goodwill'] - alldata['Intangible assets'] - alldata['Other long-term assets']\n",
    "    alldata.loc[alldata['Goodwill'].isnull(), \"Goodwill\"] = alldata['Total non-current assets'] - alldata['Net property, plant and equipment'] - alldata['Intangible assets'] - alldata['Other long-term assets']\n",
    "    alldata.loc[alldata['Intangible assets'].isnull(), \"Intangible assets\"] = alldata['Total non-current assets'] - alldata['Goodwill'] - alldata['Net property, plant and equipment'] - alldata['Other long-term assets']\n",
    "    alldata.loc[alldata['Other long-term assets'].isnull(), \"Other long-term assets\"] = alldata['Total non-current assets'] - alldata['Goodwill'] - alldata['Intangible assets'] - alldata['Net property, plant and equipment']\n",
    "    alldata.loc[alldata['Total non-current assets'].isnull(), \"Total non-current assets\"] = alldata['Net property, plant and equipment'] + alldata['Goodwill'] + alldata['Intangible assets'] + alldata['Other long-term assets']\n",
    "    alldata.loc[alldata['Total assets'].isnull(), 'Total assets']  = alldata['Total current assets'] + alldata['Total non-current assets']\n",
    "    alldata.loc[alldata['Total current assets'].isnull(), \"Total current assets\"] = alldata['Total assets'] - alldata['Total non-current assets']\n",
    "    alldata.loc[alldata['Total non-current assets'].isnull(), 'Total non-current assets'] = alldata['Total assets'] - alldata['Total current assets']\n",
    "    \n",
    "    alldata.loc[alldata['Short-term debt'].isnull(), \"Short-term debt\"] = alldata['Total current liabilities'] - alldata['Other current liabilities'] - alldata['Accrued liabilities'] - alldata['Taxes payable'] - alldata['Accounts payable']\n",
    "    alldata.loc[alldata['Accounts payable'].isnull(), \"Accounts payable\"] = alldata['Total current liabilities'] - alldata['Other current liabilities'] - alldata['Accrued liabilities'] - alldata['Taxes payable'] - alldata['Short-term debt']\n",
    "    alldata.loc[alldata['Taxes payable'].isnull(), \"Taxes payable\"] = alldata['Total current liabilities'] - alldata['Other current liabilities'] - alldata['Accrued liabilities'] - alldata['Short-term debt'] - alldata['Accounts payable']\n",
    "    alldata.loc[alldata['Accrued liabilities'].isnull(), \"Accrued liabilities\"] = alldata['Total current liabilities'] - alldata['Other current liabilities'] - alldata['Short-term debt'] - alldata['Taxes payable'] - alldata['Accounts payable']\n",
    "    alldata.loc[alldata['Other current liabilities'].isnull(), \"Other current liabilities\"] = alldata['Total current liabilities'] - alldata['Short-term debt'] - alldata['Accrued liabilities'] - alldata['Taxes payable'] - alldata['Accounts payable']\n",
    "    alldata.loc[alldata['Total current liabilities'].isnull(), \"Total current liabilities\"] = alldata['Short-term debt'] + alldata['Other current liabilities'] + alldata['Accrued liabilities'] + alldata['Taxes payable'] + alldata['Accounts payable']\n",
    "    alldata.loc[alldata['Long-term debt'].isnull(), \"Long-term debt\"] = alldata['Total non-current liabilities'] - alldata['Other long-term liabilities'] - alldata['Deferred taxes liabilities']\n",
    "    alldata.loc[alldata['Deferred taxes liabilities'].isnull(), \"Deferred taxes liabilities\"] = alldata['Total non-current liabilities'] - alldata['Other long-term liabilities'] - alldata['Long-term debt']\n",
    "    alldata.loc[alldata['Other long-term liabilities'].isnull(), \"Other long-term liabilities\"] = alldata['Total non-current liabilities'] -alldata['Long-term debt'] - alldata['Deferred taxes liabilities']\n",
    "    alldata.loc[alldata['Total non-current liabilities'].isnull(), \"Total non-current liabilities\"] = alldata['Long-term debt'] + alldata['Other long-term liabilities'] + alldata['Deferred taxes liabilities']\n",
    "    alldata.loc[alldata['Total liabilities'].isnull(), \"Total liabilities\"]  = alldata['Total current liabilities'] + alldata['Total non-current liabilities']\n",
    "    alldata.loc[alldata['Total current liabilities'].isnull(), \"Total current liabilities\"] = alldata[\"Total liabilities\"] - alldata['Total non-current liabilities']\n",
    "    alldata.loc[alldata['Total non-current liabilities'].isnull(), \"Total non-current liabilities\"] = alldata['Total liabilities'] - alldata['Total current liabilities']\n",
    "    \n",
    "    alldata.loc[alldata['Common stock'].isnull(), \"Common stock\"] = alldata[\"Total stockholders' equity\"] - alldata[\"Accumulated other comprehensive income\"] - alldata['Treasury stock'] - alldata['Retained earnings'] - alldata['Additional paid-in capital']\n",
    "    alldata.loc[alldata['Additional paid-in capital'].isnull(), \"Additional paid-in capital\"] = alldata[\"Total stockholders' equity\"] - alldata[\"Accumulated other comprehensive income\"] - alldata['Treasury stock'] - alldata['Retained earnings'] - alldata['Common stock']\n",
    "    alldata.loc[alldata['Retained earnings'].isnull(), \"Retained earnings\"] = alldata[\"Total stockholders' equity\"] - alldata[\"Accumulated other comprehensive income\"] - alldata['Treasury stock'] - alldata['Common stock'] - alldata['Additional paid-in capital']\n",
    "    alldata.loc[alldata['Treasury stock'].isnull(), \"Treasury stock\"] = alldata[\"Total stockholders' equity\"] - alldata[\"Accumulated other comprehensive income\"] - alldata['Common stock'] - alldata['Retained earnings'] - alldata['Additional paid-in capital']\n",
    "    alldata.loc[alldata['Accumulated other comprehensive income'].isnull(), \"Accumulated other comprehensive income\"] = alldata[\"Total stockholders' equity\"] - alldata[\"Common stock\"] - alldata['Treasury stock'] - alldata['Retained earnings'] - alldata['Additional paid-in capital']\n",
    "    alldata.loc[alldata[\"Total stockholders' equity\"].isnull(), \"Total stockholders' equity\"] = alldata[\"Common stock\"] + alldata[\"Accumulated other comprehensive income\"] + alldata['Treasury stock'] + alldata['Retained earnings'] + alldata['Additional paid-in capital']\n",
    "    \n",
    "    alldata.loc[alldata[\"Total liabilities and stockholders' equity\"].isnull(), \"Total liabilities and stockholders' equity\"] = alldata['Total liabilities'] + alldata[\"Total stockholders' equity\"]\n",
    "    alldata.loc[alldata[\"Total stockholders' equity\"].isnull(), \"Total stockholders' equity\"] = alldata[\"Total liabilities and stockholders' equity\"] - alldata['Total liabilities']\n",
    "    alldata.loc[alldata['Total liabilities'].isnull(), \"Total liabilities\"] = alldata[\"Total liabilities and stockholders' equity\"] - alldata[\"Total stockholders' equity\"]\n",
    "    alldata.loc[alldata[\"Total assets\"].isnull(), \"Total assets\"] = alldata[\"Total liabilities and stockholders' equity\"]\n",
    "    alldata.loc[alldata[\"Total liabilities and stockholders' equity\"].isnull(), \"Total liabilities and stockholders' equity\"] = alldata[\"Total assets\"]\n",
    "    \n",
    "    oldnullinc = curnullinc\n",
    "    curnullinc = alldata.apply(lambda s: sum(s.isnull()))[16:48].values.sum()\n",
    "    print (curnullinc)\n",
    "\n",
    "print(\"balance sheet variables done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If any incorrect values (eg. negative stockholder's equity, liabilities > assets etc., these are set to np.nan, and left for imputation)\n",
    "nonneg = set(alldata.columns.values) - set(['Other income (expense)', 'firm', 'Sector'])\n",
    "for col in nonneg:\n",
    "    alldata.loc[alldata[col] < 0, col] = np.nan\n",
    "#Special edge cases\n",
    "alldata.loc[alldata['Cost of revenue'] > alldata['Revenue'], ['Cost of revenue', 'Revenue', 'Gross profit']] = np.nan\n",
    "alldata.loc[alldata['Total assets'] < alldata['Total liabilities'], ['Total assets', 'Total liabilities']] = np.nan\n",
    "alldata.loc[alldata['Total current liabilities'] > alldata['Total liabilities'], ['Total current liabilities', 'Total non-current liabilities', 'Total liabilities']] = np.nan\n",
    "alldata.loc[alldata['Total non-current liabilities'] > alldata['Total liabilities'], ['Total current liabilities', 'Total non-current liabilities', 'Total liabilities']] = np.nan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Other calculations\n",
    "alldata.loc[alldata.Basic_EPS.isnull(), \"Basic_EPS\"] = alldata[\"Diluted_EPS\"]\n",
    "alldata.loc[alldata.Diluted_EPS.isnull(), \"Diluted_EPS\"] = alldata[\"Basic_EPS\"]\n",
    "alldata.loc[alldata.EBITDA.isnull(), \"EBITDA\"] = alldata[\"Income before taxes\"] + alldata[\"Interest Expense\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> STEP 2: Average Value Imputation - Historic Company Values and Sector Values </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Historic Company Averages\n",
    "def correct(row, col, vals):\n",
    "    if (str(row[col])=='nan') & (vals[row['firm']] != 0):\n",
    "        row[col] = vals[row['firm']]\n",
    "    return row\n",
    "\n",
    "def histcompavg (var):\n",
    "    if np.where(alldata.columns.values==var.name)[0][0] in range(1,48):\n",
    "        rev = np.cumsum(var).ffill()\n",
    "        rev = rev - (rev.where(alldata.firm != alldata.firm.shift(-1))).shift(1).ffill().fillna(0)\n",
    "        rev = rev[alldata.firm != alldata.firm.shift(-1)]\n",
    "        rev.index = alldata.firm.unique()\n",
    "        rownum = alldata.firm.value_counts()\n",
    "        rownum = rownum[alldata.firm.unique()]\n",
    "        rev = rev / rownum\n",
    "        x = pd.concat([var, alldata.firm], axis=1)\n",
    "        x = x.apply(lambda s: correct(s, var.name, rev), axis=1)\n",
    "        x[var.name] = x[var.name].apply(lambda s: round(s,2))\n",
    "        return x[var.name]\n",
    "    else:\n",
    "        return var\n",
    "\n",
    "alldata = alldata.apply(lambda s: histcompavg(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Consumer Discretionary        342\n",
       "Industrials                   329\n",
       "Financials                    329\n",
       "Information Technology        310\n",
       "Health Care                   292\n",
       "Real Estate                   153\n",
       "Energy                        152\n",
       "Consumer Staples              143\n",
       "Utilities                     140\n",
       "Materials                     119\n",
       "Telecommunication Services     15\n",
       "Name: Sector, dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Analyze sector composition\n",
    "alldata.Sector.value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nbuser/anaconda3_420/lib/python3.5/site-packages/ipykernel/__main__.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "#Derive sector averages\n",
    "#We calculate sector averages below for all income and balance sheet variables. In addition, we need to handle 3 stock variables.\n",
    "#We have exactly two firms - FOX and FOXA (Consumer Discretionary) - that do not have any stock price recordings. Given that all these\n",
    "#companies are S&P500, it is safe to assume that companies in the same industry will average out around the same stock price\n",
    "#Moreover, only 7-8 recordings are affected by the same.\n",
    "def sectavg (var):\n",
    "    if np.where(alldata.columns.values==var.name)[0][0] in np.append(np.arange(48)+1, np.array([97,98,99])):\n",
    "        industries = alldata.loc[var.isnull(), \"Sector\"].dropna().unique()\n",
    "        for ind in industries:\n",
    "            comm = np.sum(alldata.loc[alldata.Sector==ind, var.name])/ len(alldata.loc[alldata.Sector==ind, var.name])\n",
    "            var[(alldata.Sector==ind) & var.isnull()] = round(comm, 2)\n",
    "    return var\n",
    "\n",
    "alldata = alldata.apply(lambda s: sectavg(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> STEP 3: Algorithmic Imputers </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After having eliminated all missing values in balance and income statements, we need algorithmic imputers - scikitlearn's preprocessing functions or impyute's knn and mice systems, to fill the remainder of values, before proceeding to feature engineering. For this final stage of data cleaning, we divide our features into 4 buckets - noact (no imputation; a fillna to be called), calc (ratios that can easily be calculated from other completed values), knnalg (ratios that cant be calculated but are closely linked to income/balance sheet values and so nearest neighbours is an ideal approach), and simimp (ratios that are finally not closely linked to available values and must be directly imputed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Declare the final method of imputation for key metrics variables, mostly existing imputation algorithms\n",
    "#These variables determine the response indicator and thus cannot be imputed\n",
    "noact = ['Dividend Yield', 'Payout Ratio']\n",
    "#These are calculated using known financial formulae\n",
    "calc = ['Net Debt to EBITDA', 'SG&A to Revenue', 'Debt to Assets', \n",
    "        'Intangibles to Total Assets', 'R&D to Revenue']\n",
    "#These are closely linked to values we calculated above and thus nearest neighbour is apt\n",
    "knnalg = ['Average Inventory','Average Payables', 'Average Receivables', 'Book Value per Share',\n",
    "          'Capex per Share', 'Capex to Depreciation', 'Days Payables Outstanding', \n",
    "          'Days of Inventory on Hand','Inventory Turnover','Invested Capital', 'Market Cap', \n",
    "          'Net Current Asset Value','Payables Turnover']\n",
    "#We do not have enough information to closely match or calculate these already and thus use Imputer\n",
    "simimp = ['Revenue per Share', 'Shareholders Equity per Share','EV to Operating cash flow',\n",
    "          'Stock-based compensation to Revenue','Tangible Book Value per Share', \n",
    "          'Capex to Operating Cash Flow','Capex to Revenue','Cash per Share',\n",
    "          'Days Sales Outstanding','EV to Free cash flow','EV to Sales','Earnings Yield',\n",
    "          'Enterprise Value','Enterprise Value over EBITDA','Free Cash Flow Yield', \n",
    "          'Free Cash Flow per Share','Graham Net-Net','Graham Number','Income Quality',\n",
    "          'Interest Coverage','Interest Debt per Share','Net Income per Share',\n",
    "          'Operating Cash Flow per Share','PB ratio','PE ratio','PFCF ratio', 'POCF ratio', \n",
    "          'PTB ratio','Price to Sales Ratio']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Deal with noact and calc first\n",
    "for i in noact:\n",
    "    alldata[i] = alldata[i].fillna(0)\n",
    "alldata.loc[alldata[calc[0]].isnull(), calc[0]] = alldata['Total liabilities'] / alldata['EBITDA']\n",
    "alldata.loc[alldata[calc[1]].isnull(), calc[1]] = alldata['Sales, General and administrative'] / alldata['Revenue']\n",
    "alldata.loc[alldata[calc[2]].isnull(), calc[2]] = alldata['Total liabilities'] / alldata['Total assets']\n",
    "alldata.loc[alldata[calc[3]].isnull(), calc[3]] = alldata['Intangible assets'] / alldata['Total assets']\n",
    "alldata.loc[alldata[calc[4]].isnull(), calc[4]] = alldata['Research and development'] / alldata['Revenue']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Handle knnalg imputations\n",
    "relevdata = alldata[np.append(alldata.columns.values[1:48], knnalg)]\n",
    "relevdata = fast_knn(relevdata, k=5)\n",
    "alldata.drop(knnalg, axis=1, inplace=True)\n",
    "relevdata = relevdata.iloc[:, 47:]\n",
    "cur = relevdata.columns.values\n",
    "relevdata.rename(columns=dict(zip(cur, knnalg)), inplace=True)\n",
    "alldata = pd.concat([alldata, relevdata], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAABvCAYAAADlohdcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4xLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvAOZPmwAADNJJREFUeJzt3X+QXWV9x/H3Jwkk/FKSJtCQYBYkUumvwKQQbDumgglQMMxUO0YtW6DNtNJRW9tKrDWNP2a0Aw06ZUCqEfAHgqg1jQwBU2hnaEGS2tJIwCwSyJKEbEqIEGgl8u0fz3PJcdns3t3s3rv3PJ/XzJ17z3Oee87zvc/O+d7nOeeeVURgZmblmdDuBpiZWXs4AZiZFcoJwMysUE4AZmaFcgIwMyuUE4CZWaGcAMw6kKStks5tdzusszkB2JjIB6gXJT0vaaekGyUd3e52dQpJh0u6WlJv/gwfl7Sq3e2yenECsLF0UUQcDcwDTgeWt7k945KkSQMULwfmA2cCxwC/BXx/DPYtST4OFModb2MuInYC60iJAABJkyVdJelJSU9Lul7SEXndZkkXVupOkrRb0hl5eYGkf5P0rKT/krSwUvdeSR+XdJ+k5yTdJWl6XrdQUm+1bdWpFEkTJF0p6TFJ/yPpNknTBoqpsS1JH85t2yrp3U3G13jvhyTtBL44wC5+DfhWRGyPZGtE3NyvzjxJD0naK+lWSVPy9qdKWiupT9Ke/Hp2v8/ok5LuA14ATpb0WklfkLRD0lOSPiFp4kG61GrCCcDGXD74nA/0VIo/DbyBlBROAWYBH83rbgGWVuouBnZHxH9ImgV8B/gEMA34c+AbkmZU6r8LuBQ4Djg812nG+4CLgTcDJwB7gGsHqf/zwPTc9m7gBkmnNhFf473TgDnAsgG2fT/wZ5LeK+mXJWmAOr8LnAecBPwK8Pu5fAIpqcwBXge8CPx9v/f+Xt7vMcATwE3A/tzW04FFwB8MErvVQUT44ceoP4CtwPPAc0AA64Fj8zoB+4DXV+qfDTyeX5+S33dkXv4K8NH8+kPAl/rtax3QnV/fC3yksu69wJ359UKgd4B2nptfbwbOqaybCbwETBogvoWkA+ZRlbLbgL9uIr6FwE+AKYN8fhOBK4D7gP8DtjdirLT7PZXlvwWuP8i25gF7Ksv3Ah+rLB+f93FEpWwpcE+7/478GNvHQHOPZqPl4oj4rqQ3A18lfVt+FpgBHAlsrHyxFemgR0T0SNoMXCTpn4C3kb6VQvpW+w5JF1X2cxhwT2V5Z+X1C0CzJ5/nAN+S9HKl7KekA+RTA9TfExH7KstPkEYOg8aX9UXE/x6sIRHxU9Lo49o8dXQZsFrS9yJic67WP84TACQdCawijQ6m5vXHSJqYtwuwrV/chwE7Ku2d0K+O1ZATgI25iPgXSTcCV5GmWHaTpiV+MSIGOrDCgWmgCcDDEdGYPtpGGgH84Qiaso90YAYgz3FXp462AZdFxH1Nbm+qpKMqSeB1wCaai6/p2/BGxIukRLASOI00UhnMB4FTgbMiYqekeaQTyNVppOr+t5FGANMjYn+z7bLO53MA1irXAG+VNC8iXgb+AVgl6TgASbMkLa7U/xppHvqPSaOHhi+TRgaLJU2UNCWfVJ3N0H4ITJH025IOAz4CTK6svx74pKQ5uU0zJC0ZYpsr8yWbvwlcCHy9yfgGJekDOa4j8knwbtJ8fTNXAh1DSkDP5pPYKwarHBE7gLuAqyW9Jp8Mf30euVmNOQFYS0REH3AzaY4c0lx+D3C/pB8D3yV9a23U3wH8O/Am4NZK+TZgCfBhoI/07fUvaOJvOSL2ks4JfJ40pbMPqF4V9BlgDXCXpOdIJ2LPGmSTO0knireTzlP8UUQ80kx8TXgRuDrvYzfpfMDvRMSPmnjvNcAR+X33A3c28Z5LSCfMHybFdDvpHIjVmCL8D2HMhitfevrliGhm5GE2LnkEYGZWKCcAM7NCeQrIzKxQHgGYmRXKCcDMrFDj+odg06dPj66urnY3w8yso2zcuHF3RMwYqt64TgBdXV1s2LCh3c0wM+sokp5opp6ngMzMCjWuRwCHSisHuoPuyMUKXzFlZvXhEYCZWaGcAMzMCuUEYGZWKCcAM7NCOQGYmRXKCcDMrFBOAGZmhXICMDMrlBOAmVmhnADMzArlBGBmVignADOzQjkBmJkVygnAzKxQTgBmZoVyAjAzK5QTgJlZoZwAzMwK5QRgZlYoJwAzs0I5AZiZFcoJwMysUE4AZmaFGjIBSFotaZekTZWyaZLulrQlP0/N5ZL0WUk9kh6SdEblPd25/hZJ3WMTjpmZNauZEcCNwHn9yq4E1kfEXGB9XgY4H5ibH8uA6yAlDGAFcBZwJrCikTTMzKw9hkwAEfGvwDP9ipcAN+XXNwEXV8pvjuR+4FhJM4HFwN0R8UxE7AHu5tVJxczMWmik5wCOj4gdAPn5uFw+C9hWqdebyw5WbmZmbTLaJ4E1QFkMUv7qDUjLJG2QtKGvr29UG2dmZgeMNAE8nad2yM+7cnkvcGKl3mxg+yDlrxIRN0TE/IiYP2PGjBE2z8zMhjLSBLAGaFzJ0w18u1J+Sb4aaAGwN08RrQMWSZqaT/4uymVmZtYmk4aqIOkWYCEwXVIv6WqeTwG3SboceBJ4R65+B3AB0AO8AFwKEBHPSPo48GCu97GI6H9i2czMWmjIBBARSw+y6pwB6gZwxUG2sxpYPazWmZnZmPEvgc3MCuUEYGZWKCcAM7NCOQGYmRXKCcDMrFBOAGZmhXICMDMrlBOAmVmhnADMzArlBGBmVignADOzQjkBmJkVygnAzKxQTgBmZoVyAjAzK5QTgJlZoZwAzMwK5QRgZlYoJwAzs0IN+T+B7QCt1KhuL1bEqG7PzGw4PAIwMyuUE4CZWaGcAMzMCuUEYGZWqJYnAEnnSXpUUo+kK1u9fzMzS1p6FZCkicC1wFuBXuBBSWsi4uFWtmO88FVFZtZOrR4BnAn0RMSPIuInwNeAJS1ug5mZ0frfAcwCtlWWe4GzWtyG2hrtEcVY8CjFbPxodQIY6Aj1M0cEScuAZXnxeUmPjnBf04HdI3xvpxr3MetvRj1JjfuYx0BpMZcWLxx6zHOaqdTqBNALnFhZng1sr1aIiBuAGw51R5I2RMT8Q91OJ3HMZSgt5tLihdbF3OpzAA8CcyWdJOlw4J3Amha3wczMaPEIICL2S/oTYB0wEVgdET9oZRvMzCxp+c3gIuIO4I4W7OqQp5E6kGMuQ2kxlxYvtChmRfiqDDOzEvlWEGZmhapdAqjrrSYknSjpHkmbJf1A0vtz+TRJd0vakp+n5nJJ+mz+HB6SdEZ7Ixg5SRMlfV/S2rx8kqQHcsy35gsKkDQ5L/fk9V3tbPdISTpW0u2SHsn9fXbd+1nSn+a/602SbpE0pW79LGm1pF2SNlXKht2vkrpz/S2Sug+lTbVKAJVbTZwPnAYslXRae1s1avYDH4yINwILgCtybFcC6yNiLrA+L0P6DObmxzLgutY3edS8H9hcWf40sCrHvAe4PJdfDuyJiFOAVbleJ/oMcGdE/ALwq6TYa9vPkmYB7wPmR8QvkS4QeSf16+cbgfP6lQ2rXyVNA1aQfkB7JrCikTRGJCJq8wDOBtZVlpcDy9vdrjGK9dukeyo9CszMZTOBR/PrzwFLK/VfqddJD9JvRdYDbwHWkn5MuBuY1L/PSVeXnZ1fT8r11O4Yhhnva4DH+7e7zv3MgTsETMv9thZYXMd+BrqATSPtV2Ap8LlK+c/UG+6jViMABr7VxKw2tWXM5CHv6cADwPERsQMgPx+Xq9Xls7gG+Evg5bz8c8CzEbE/L1fjeiXmvH5vrt9JTgb6gC/maa/PSzqKGvdzRDwFXAU8Cewg9dtG6t3PDcPt11Ht77olgCFvNdHpJB0NfAP4QET8eLCqA5R11Gch6UJgV0RsrBYPUDWaWNcpJgFnANdFxOnAPg5MCwyk42POUxhLgJOAE4CjSFMg/dWpn4dysBhHNfa6JYAhbzXRySQdRjr4fyUivpmLn5Y0M6+fCezK5XX4LH4deJukraQ7x76FNCI4VlLjNyzVuF6JOa9/LfBMKxs8CnqB3oh4IC/fTkoIde7nc4HHI6IvIl4Cvgm8iXr3c8Nw+3VU+7tuCaC2t5qQJOALwOaI+LvKqjVA40qAbtK5gUb5JflqggXA3sZQs1NExPKImB0RXaS+/OeIeDdwD/D2XK1/zI3P4u25fkd9M4yIncA2SafmonOAh6lxP5OmfhZIOjL/nTdirm0/Vwy3X9cBiyRNzSOnRblsZNp9UmQMTrJcAPwQeAz4q3a3ZxTj+g3SUO8h4D/z4wLS3Od6YEt+npbri3RF1GPAf5OusGh7HIcQ/0JgbX59MvA9oAf4OjA5l0/Jyz15/cntbvcIY50HbMh9/Y/A1Lr3M7ASeATYBHwJmFy3fgZuIZ3jeIn0Tf7ykfQrcFmOvQe49FDa5F8Cm5kVqm5TQGZm1iQnADOzQjkBmJkVygnAzKxQTgBmZoVyAjAzK5QTgJlZoZwAzMwK9f/U/Mv+I+LMfAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7112b07e48>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7113efc6d8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7112ead4a8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Explore feasible method of simple imputation based on value distribution\n",
    "#Here, we choose three random variables and plot these to get a general idea\n",
    "plt.subplot(311)\n",
    "plt.hist(alldata[simimp[0]].sort_values().dropna().values, bins=15, color='green')\n",
    "plt.title(simimp[0])\n",
    "plt.show()\n",
    "plt.subplot(312)\n",
    "plt.hist(alldata[simimp[4]].sort_values().dropna().values, bins=15, color='green')\n",
    "plt.title(simimp[4])\n",
    "plt.show()\n",
    "plt.subplot(313)\n",
    "plt.hist(alldata[simimp[10]].sort_values().dropna().values, bins=15, color='green')\n",
    "plt.title(simimp[10])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the distributions are clearly right-skewed, using 'mean' approach to impute missing values would likely lead to general over-estimates. Thus, since the data is not normal, we have two approaches: either power-transform the data into a normal distribution and then impute with mean, or just directly impute with median. To preserve the actual current values for the variables, the latter approach has been chosen. Therefore, scikitlearn's Imputer is used with the 'median' strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nbuser/anaconda3_420/lib/python3.5/site-packages/sklearn/utils/deprecation.py:66: DeprecationWarning: Class Imputer is deprecated; Imputer was deprecated in version 0.20 and will be removed in 0.22. Import impute.SimpleImputer from sklearn instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "imp = Imputer(missing_values=np.nan, strategy='median')\n",
    "for col in simimp:\n",
    "    vals = alldata[col].values.reshape(-1,1)\n",
    "    newvals = imp.fit_transform(vals).reshape(1,-1)\n",
    "    alldata[col] = newvals[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], dtype: float64)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#All missing values have been successfully imputed\n",
    "#Save cleaned file before proceeding to feature engineering\n",
    "alldata.to_pickle('sp500cleandata.pkl')\n",
    "alldata.apply(lambda s: sum(s.isnull()) if sum(s.isnull()) > 0 else np.nan).dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Feature Engineering </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we engineer three varied buckets of features for the data - dividend values and attributes (amount, frequency and distance from payment), company characteristics (composition in the SP500 or industry on the basis of market cap etc.), and numerous financial ratios. This helps balance 'static' and 'dynamic' variables in our final data,  making it more representative and predictive of our response variable. After feature engineering, we proceed to model training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> STEP 1: Dividend-Based Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dividends can be calculated as either Payout Ratio * Earnings per Share or Dividend Yield * Stock Price. While these mathematically-yield close results, they cause a small an average difference of 0.13 cents on our data. Hence, the dividend value used in model training is the average of these two approaches. Subsequently, after dynamic dividend features are engineered, the payout ratio and yield are dropped as these cannot be known prior to dividend declaration and thus have no net predictive or applicable value in our pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "alldata = pd.read_pickle('../data/sp500cleandata.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.09035920830895001\n"
     ]
    }
   ],
   "source": [
    "x = alldata['Dividend Yield'] * alldata['stockprc']\n",
    "y = alldata['Payout Ratio'] * alldata['Basic_EPS']\n",
    "#Average error in two approaches to dividend\n",
    "print(sum(x-y)/len(x))\n",
    "alldata['Dividend'] = (x + y)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PAID      1873\n",
       "UNPAID     451\n",
       "Name: Dividend, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alldata.Dividend.apply(lambda s: 'PAID'  if s > 0 else 'UNPAID').value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dynamic payment duration variables\n",
    "alldata['Div_Paid?'] = alldata.Dividend.apply(lambda s: 1 if s > 0 else 0)\n",
    "#When attempting to check whether the company has paid dividend in the previous year, we would have to drop the first recording for\n",
    "#every firm since such data would be unavailable. However, since we already have limited data available from the API, it would \n",
    "#significantly reduce our training set and affect the applicability of the model. To get around it, we derive estimations for this\n",
    "#first recording. As such, we assign a 1 (last dividend paid) if the company has paid a special majority of more dividends \n",
    "#than not in recorded history, else 0. The reason we are stricter at a 0.67 requirement than 0.5 is to ensure greater certainty in\n",
    "#the fundamental dividend features in our model\n",
    "row = np.cumsum(pd.Series(np.ones(len(alldata))))\n",
    "track = np.cumsum(alldata['Div_Paid?'])\n",
    "row = row - row.where(alldata.firm != alldata.firm.shift(-1)).shift(1).ffill().fillna(0)\n",
    "track = track - track.where(alldata.firm != alldata.firm.shift(-1)).shift(1).ffill().fillna(0)\n",
    "row = row.where(alldata.firm != alldata.firm.shift(-1)).bfill()\n",
    "track = track.where(alldata.firm != alldata.firm.shift(-1)).bfill()\n",
    "track = track/row\n",
    "track = track.where(alldata.firm != alldata.firm.shift(1)).fillna(0)\n",
    "track = track.apply(lambda s: 1 if s > 0.67 else 0)\n",
    "alldata['Paid_LastYr?'] = alldata['Div_Paid?'].shift(1)\n",
    "alldata.loc[alldata.firm != alldata.firm.shift(1), 'Paid_LastYr?'] = 0\n",
    "alldata['Paid_LastYr?'] = alldata['Paid_LastYr?'] + track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We now define a categorical variable - distance from last dividend. It can either be 1-5 or UNKNOWN. The reason this is being treated\n",
    "#as discrete not continuous is to allow the fact that since companies have existed for long durations, imputing unavailable data may lead\n",
    "#to excessive engineering of the training set, not being representative of the original data itself.\n",
    "dist = alldata['Paid_LastYr?'].map({1:0, 0:1})\n",
    "dist = np.cumsum(dist)\n",
    "dist = dist - dist.where(alldata.firm != alldata.firm.shift(-1)).shift(1).ffill().fillna(0)\n",
    "alldata['DistanceFromLast'] = dist + 1\n",
    "alldata['DistanceFromLast'] = alldata['DistanceFromLast'].astype('int').astype('str')\n",
    "alldata.loc[alldata.firm != alldata.firm.shift(1), \"DistanceFromLast\"] = 'UNKNOWN'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now try to find the proportion of industry that paid last year. Since the bulk of our data lies in 2014-2018, these years have enough samples to provide reliable estimates for this value. However, 2013 and 2019, seen below, have very few samples. Thus, we impute 2014's readings to 2013, and 2018's readings to 2019. This should not severely affect the model, given only 13 recordings total in consideration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2017    486\n",
       "2016    473\n",
       "2018    463\n",
       "2015    458\n",
       "2014    431\n",
       "2013      8\n",
       "2019      5\n",
       "Name: Year, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alldata['Year'] = alldata['date'].apply(lambda s: int(s/100))\n",
    "alldata.drop(['date'], axis=1, inplace=True)\n",
    "alldata.Year.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/conda/envs/notebook/lib/python3.7/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/srv/conda/envs/notebook/lib/python3.7/site-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/srv/conda/envs/notebook/lib/python3.7/site-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  import sys\n",
      "/srv/conda/envs/notebook/lib/python3.7/site-packages/ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/srv/conda/envs/notebook/lib/python3.7/site-packages/ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "/srv/conda/envs/notebook/lib/python3.7/site-packages/pandas/core/frame.py:3940: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  errors=errors)\n",
      "/srv/conda/envs/notebook/lib/python3.7/site-packages/ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  del sys.path[0]\n",
      "/srv/conda/envs/notebook/lib/python3.7/site-packages/pandas/core/indexing.py:543: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n"
     ]
    }
   ],
   "source": [
    "#We first create a table with the year and sector wise cumulations of paid_lastyr? to determine prop of sector that paid\n",
    "vals = alldata[['Year', 'Sector', 'Paid_LastYr?']]\n",
    "vals.sort_values(by=['Sector', 'Year'], ascending=True, inplace=True)\n",
    "vals.reset_index(drop=True, inplace=True)\n",
    "#Since the data is first sorted on sector, we can use year as the divider between individual cumulative sums\n",
    "vals['Total'] = pd.Series(np.arange(len(vals)) + 1)\n",
    "vals['Total'] = vals['Total'] - vals['Total'].where(vals.Year != vals.Year.shift(-1)).shift(1).ffill().fillna(0)\n",
    "tmp = np.cumsum(vals['Paid_LastYr?'])\n",
    "tmp = tmp - tmp.where(vals.Year != vals.Year.shift(-1)).shift(1).ffill().fillna(0)\n",
    "vals['Paid_LastYr?'] = tmp\n",
    "vals['SectPaidLastYr'] = vals['Paid_LastYr?'] / vals['Total']\n",
    "vals.drop(['Paid_LastYr?', 'Total'], axis=1, inplace=True)\n",
    "vals.drop_duplicates(subset=['Sector', 'Year'], keep='last', inplace=True)\n",
    "vals.loc[vals.Year==2013, 'SectPaidLastYr'] = vals.SectPaidLastYr.shift(-1)\n",
    "vals.loc[vals.Year==2019, 'SectPaidLastYr'] = vals.SectPaidLastYr.shift(1)\n",
    "#Using the above table, we add the feature to alldata\n",
    "alldata = alldata.merge(vals, on=['Sector', 'Year'], how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> STEP 2: Company Characteristics </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is simply one major company characteristics we calculate based on market cap: the proportion of a company to average in its industry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/conda/envs/notebook/lib/python3.7/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/srv/conda/envs/notebook/lib/python3.7/site-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/srv/conda/envs/notebook/lib/python3.7/site-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  import sys\n",
      "/srv/conda/envs/notebook/lib/python3.7/site-packages/ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/srv/conda/envs/notebook/lib/python3.7/site-packages/ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "/srv/conda/envs/notebook/lib/python3.7/site-packages/ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  del sys.path[0]\n"
     ]
    }
   ],
   "source": [
    "#We first create a table with the year and sector wise cumulations of market cap to determine ratio of company to avg yearly mktcap\n",
    "vals = alldata[['Year', 'Sector', 'Market Cap']]\n",
    "vals.sort_values(by=['Sector', 'Year'], ascending=True, inplace=True)\n",
    "vals.reset_index(drop=True, inplace=True)\n",
    "#Since the data is first sorted on sector, we can use year as the divider between individual cumulative sums\n",
    "vals['Total'] = pd.Series(np.arange(len(vals)) + 1)\n",
    "vals['Total'] = vals['Total'] - vals['Total'].where(vals.Year != vals.Year.shift(-1)).shift(1).ffill().fillna(0)\n",
    "tmp = np.cumsum(vals['Market Cap'])\n",
    "tmp = tmp - tmp.where(vals.Year != vals.Year.shift(-1)).shift(1).ffill().fillna(0)\n",
    "vals['Market Cap'] = tmp\n",
    "vals['AvgMktCap'] = vals['Market Cap'] / vals['Total']\n",
    "vals.drop(['Market Cap', 'Total'], axis=1, inplace=True)\n",
    "vals.drop_duplicates(subset=['Sector', 'Year'], keep='last', inplace=True)\n",
    "vals.loc[vals.Year==2013, 'AvgMktCap'] = vals.AvgMktCap.shift(-1)\n",
    "vals.loc[vals.Year==2019, 'AvgMktCap'] = vals.AvgMktCap.shift(1)\n",
    "alldata = alldata.merge(vals, on=['Sector', 'Year'], how='left')\n",
    "alldata['PropSectorAvgMktCap'] = alldata['Market Cap']/alldata['AvgMktCap']\n",
    "alldata.drop(['AvgMktCap'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> STEP 3: Financial Ratios </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We proceed to engineer limited financial ratios from the data available above. These help add greater depth to the variety of financial information available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "alldata['currentRatio'] = alldata['Total current assets'] / alldata['Total current liabilities']\n",
    "alldata['grossProfitMargin'] = alldata['Gross profit'] / alldata['Revenue']\n",
    "alldata['operatingProfitMargin'] = alldata['Operating income'] / alldata['Revenue']\n",
    "alldata['returnOnAssets'] = alldata['Net income'] / alldata['Total assets']\n",
    "alldata['assetTurnover'] = alldata['Revenue'] / alldata['Total assets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2092 entries, 0 to 2091\n",
      "Columns: 118 entries, Revenue to assetTurnover\n",
      "dtypes: float64(113), int64(2), object(3)\n",
      "memory usage: 1.9+ MB\n"
     ]
    }
   ],
   "source": [
    "#Drop errenous values\n",
    "#Due to the imputation and engineering performed in this section, there are a few cases where data is excessively manipulated such that\n",
    "#it does not hold logical coherence any more. We did not exclude them before feature engineering as we needed them for sector-related\n",
    "#feature engineering. These data points are eliminated below\n",
    "firm = set(alldata.loc[alldata['Cost of revenue'] > alldata['Revenue'], \"firm\"])\n",
    "firm = firm.union(set(alldata.loc[alldata['Total current liabilities'] > alldata['Total liabilities'], \"firm\"]))\n",
    "firm = firm.union(set(alldata.loc[alldata['Total non-current liabilities'] > alldata['Total liabilities'], \"firm\"]))\n",
    "firm = firm.union(set(alldata.loc[alldata['Total current assets'] > alldata['Total assets'], \"firm\"]))\n",
    "firm = firm.union(set(alldata.loc[alldata['Total non-current assets'] > alldata['Total assets'], \"firm\"]))\n",
    "alldata = alldata[(~(alldata.firm.isin(firm)))]\n",
    "alldata.sort_values(by=['firm', 'Year'], ascending=[True, True], inplace=True)\n",
    "alldata.reset_index(drop=True, inplace=True)\n",
    "alldata.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "alldata.to_pickle('../data/sp500finaldata.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
